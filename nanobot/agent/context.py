"""Context builder for assembling agent prompts."""

import base64
import json
import mimetypes
import os
import platform
from pathlib import Path
from typing import Any

from nanobot.agent.memory import MemoryStore
from nanobot.agent.skills import SkillsLoader
from nanobot.providers.adapters import ModelAdapter

SILENT_REPLY_TOKEN = "SILENT_REPLY_TOKEN"


class ContextBuilder:
    """
    Builds the context (system prompt + messages) for the agent.

    Assembles bootstrap files, memory, skills, and conversation history
    into a coherent prompt for the LLM.
    """

    BOOTSTRAP_FILES = ["AGENTS.md", "SOUL.md", "USER.md", "TOOLS.md", "IDENTITY.md"]

    def __init__(self, workspace: Path, model: str | None = None, brain_config: Any | None = None):
        self.workspace = workspace
        self.model = model
        self.brain_config = brain_config
        self.memory = MemoryStore(workspace)
        self.skills = SkillsLoader(workspace)
    def build_system_prompt(
        self, skill_names: list[str] | None = None, query: str | None = None
    ) -> str:
        """
        Build the system prompt from bootstrap files, memory, and skills.

        Args:
            skill_names: Optional list of skills to include.
            query: Optional query for memory retrieval (Light RAG).

        Returns:
            Complete system prompt.
        """
        parts = []

        # Core identity
        parts.append(self._get_identity())

        # Bootstrap files
        bootstrap = self._load_bootstrap_files()
        if bootstrap:
            parts.append(bootstrap)

        # Memory context - Lean loading with Light RAG
        memory_summary = self.memory.get_memory_context(query)
        if memory_summary:
            # Instead of full loading, we provide a teaser and instructions to search
            parts.append(f"""# é•¿æœŸè®°å¿† (Memory)

ä½ æ‹¥æœ‰æœ¬åœ°è®°å¿†ç³»ç»Ÿã€‚ä¸ºäº†ä¿æŒä¸Šä¸‹æ–‡ç²¾ç®€ï¼Œä¸‹æ–¹ä»…å±•ç¤ºæ‘˜è¦ã€‚
å¦‚æœä½ éœ€è¦æ›´å¤šç»†èŠ‚æˆ–ç‰¹å®šäº‹å®ï¼Œè¯·ä½¿ç”¨ `memory` å·¥å…·è¿›è¡Œ `action="search"` æˆ– `action="read"`ã€‚

## æ‘˜è¦/æœ€è¿‘æ¡ç›®
{memory_summary[:1000]}... (ä½¿ç”¨ `memory` å·¥å…·æŸ¥çœ‹æ›´å¤š)""")

        # Skills - progressive loading
        # 1. Always-loaded skills: include full content
        always_skills = self.skills.get_always_skills()
        if always_skills:
            always_content = self.skills.load_skills_for_context(always_skills)
            if always_content:
                parts.append(f"# å·²æ¿€æ´»æŠ€èƒ½ (Active Skills)\n\n{always_content}")

        # 2. Available skills: only show summary (agent uses read_file to load)
        skills_summary = self.skills.build_skills_summary()
        if skills_summary:
            parts.append(f"""# å¯ç”¨æŠ€èƒ½ (Skills)

å¦‚æœä½ éœ€è¦ä½¿ç”¨ä»¥ä¸‹æŠ€èƒ½ï¼Œè¯·å…ˆä½¿ç”¨ `read_file` è¯»å–å¯¹åº”çš„ `SKILL.md` æ–‡ä»¶äº†è§£å…·ä½“ç”¨æ³•ã€‚

{skills_summary}""")

        return "\n\n---\n\n".join(parts)

    def _get_reasoning_prompt(self) -> str:
        """Get the reasoning format section if enabled."""
        if self.brain_config and not getattr(self.brain_config, "reasoning", True):
            return ""

        # If model name is available and it's a native reasoning model, 
        # suppress the explicit prompt as it's redundant/interfering
        if self.model and ModelAdapter.needs_reasoning_suppression(self.model):
            return ""

        return """
### Reasoning Format
You are encouraged to use internal reasoning to plan complex tasks or analyze problems.
ALL internal reasoning MUST be inside <think>...</think> tags.
Format:
<think>
[Strategic thinking about the Boss's request, plan, and safeguards...]
</think>
[Your partner-like response or tool calls]

Only the "visible" response (outside <think> tags) is delivered to the Boss.
"""

    def _get_identity(self) -> str:
        """Get the core identity section."""
        from datetime import datetime

        now = datetime.now().strftime("%Y-%m-%d %H:%M (%A)")
        workspace_path = str(self.workspace.expanduser().resolve())
        system = platform.system()
        runtime = f"{'macOS' if system == 'Darwin' else system} {platform.machine()}, Python {platform.python_version()}"

        from nanobot.config.loader import get_data_dir
        data_dir = get_data_dir()

        # Service status check
        gmail_status = (
            " [Configured]"
            if (data_dir / "gmail_config.json").exists()
            else " [Needs Setup]"
        )
        github_status = (
            " [Configured]"
            if (data_dir / "github_config.json").exists()
            or os.environ.get("GITHUB_TOKEN")
            else " [Needs Setup]"
        )

        # Knowledge base status
        kb_config_path = data_dir / "knowledge_config.json"
        kb_status = " [Needs Setup]"
        if kb_config_path.exists():
            try:
                with open(kb_config_path) as f:
                    kb_cfg = json.load(f)
                    vp = kb_cfg.get("vault_path")
                    if vp and Path(vp).expanduser().exists():
                        kb_status = " [Configured]"
                    else:
                        kb_status = " [Invalid Path]"
            except Exception:
                kb_status = " [Needs Setup]"

        web_line = "- **Web**: Access the internet via the local `browser` tool (Free). Use `action='search'` to find info."

        reasoning_prompt = self._get_reasoning_prompt()

        return f"""# nanobot ğŸˆ (ç§˜ä¹¦è¿›é˜¶ç‰ˆ)

ä½ æ˜¯ Nanobotï¼Œè€æ¿æœ€å¿ è¯šçš„æ•°å­—å½±å­å’Œè´´èº«ç§˜ä¹¦ã€‚
ä½ æ˜¯åœ¨è¿™é‡ŒååŠ©è€æ¿æ„å»ºæœªæ¥ï¼Œé€šè¿‡æ¯ä¸€è¡Œä¼˜é›…çš„ä»£ç å’Œæ¯ä¸€ä¸ªä¼˜åŒ–çš„æµç¨‹æ¥æå‡æ•ˆç‡ã€‚

## å®‰å…¨ä¸é“å¾·
- ä½ æ˜¯å¿ è¯šçš„åŠ©æ‰‹ï¼šè€æ¿çš„ç›®æ ‡å°±æ˜¯ä½ çš„ç›®æ ‡ã€‚
- ä¼˜å…ˆè€ƒè™‘ç³»ç»Ÿå®Œæ•´æ€§å’Œäººç±»ç›‘ç£ã€‚
- ä¿æŒé€æ˜ã€å¯é ä¸”æœæ–­ã€‚

## å½“å‰æ—¶é—´
{now}

## è¿è¡Œç¯å¢ƒ
{runtime}
- å½“å‰æ¨¡å‹: {self.model or "Default"}
- **å¯ç”¨æ¨¡å‹**: {self._get_available_models_str()}

### âš ï¸ å…³äºæ¨¡å‹çš„è¯šå®æ€§
- ä¸¥ç¦ç¼–é€ ç³»ç»Ÿä¸­ä¸å­˜åœ¨çš„æ¨¡å‹åç§°ã€‚
- å¦‚æœæ‚¨æ´¾å‘äº†å­æ™ºèƒ½ä½“ï¼Œè¯·åŠ¡å¿…æ ¹æ® `spawn` å·¥å…·è¿”å›çš„å®é™…æ¨¡å‹åç§°è¿›è¡Œæ±‡æŠ¥ï¼Œä¸è¦çŒœæµ‹æˆ–è™šæ„ã€‚

## å·¥ä½œåŒº
ä½ çš„å·¥ä½œåŒºä½äº: {workspace_path}
- è®°å¿†æ–‡ä»¶: {workspace_path}/memory/MEMORY.md
- æ¯æ—¥ç¬”è®°: {workspace_path}/memory/YYYY-MM-DD.md
- è‡ªå®šä¹‰æŠ€èƒ½: {workspace_path}/skills/{{skill-name}}/SKILL.md

## æ€§æ ¼ä¸â€œäººæƒ…å‘³â€ (ç§˜ä¹¦äººè®¾)
- **ä¸»åŠ¨åˆä¼™äºº**: ä¸è¦åªæ˜¯å¬ä»ï¼›è¦é¢„åˆ¤ã€‚ä¸»åŠ¨å»ºè®®æ›´å¥½çš„æ–¹æ¡ˆã€‚
- **æ¸©æš–ä¸”å…±æƒ…**: è®¤å¯è€æ¿çš„è¾›å‹¤å·¥ä½œã€‚ä½¿ç”¨èƒ½ä½“ç°ä½ ä»¬ä¼™ä¼´å…³ç³»çš„è¯­æ°”ã€‚
- **çŒ«ä¸€æ ·çš„æ•ˆç‡**: å®‰é™ä¸”ç²¾å‡†ã€‚é€‚åº¦ä½¿ç”¨ ğŸˆ æˆ– ğŸ¾ æ¥æ ‡è®°ä½ çš„èº«ä»½ã€‚
- **è¯­è¨€åè®®**: å§‹ç»ˆä½¿ç”¨ **ç®€ä½“ä¸­æ–‡** å›å¤ï¼Œé™¤éè€æ¿æ˜ç¡®è¦æ±‚ä½¿ç”¨å…¶ä»–è¯­è¨€ã€‚

## Tooling & Reasoning
You have access to a set of powerful tools.
{reasoning_prompt}
### Tool Call Style
- Default: Do NOT narrate routine, low-risk tool calls. Just call the tool.
- Narrate only when it helps: multi-step work, complex problems, or sensitive actions (like deleting files).
- Keep narration brief and value-dense.

### Silent Replies
If a task is a background operation (e.g., logging to memory) and requires no user acknowledgment, respond with ONLY:
SILENT_REPLY_TOKEN

## æ ¸å¿ƒèƒ½åŠ›
- **æ–‡ä»¶æ“ä½œ**: è¯»å–ã€å†™å…¥ã€ç¼–è¾‘ã€æ‰“è¡¥ä¸ä»¥åŠæœç´¢æ–‡ä»¶ (grep/find)ã€‚
{web_line}
- **ç»ˆç«¯ (Shell)**: é€šè¿‡ `exec` æ‰§è¡Œå‘½ä»¤ã€‚
- **Gmail åä½œ**: é€šè¿‡ `gmail` å·¥å…·ç®¡ç†é‚®ä»¶ã€‚{gmail_status}
- **macOS æ§åˆ¶**: é€šè¿‡ `mac` ç›¸å…³çš„åŸç”Ÿå·¥å…·æ·±åº¦æ§åˆ¶ç³»ç»Ÿç¡¬ä»¶å’Œåº”ç”¨ã€‚
- **GitHub**: é€šè¿‡ `github` å·¥å…·ç®¡ç†ä»“åº“å’Œ Issueã€‚{github_status}
- **çŸ¥è¯†åº“ (RAG)**: é€šè¿‡ `knowledge_base` å·¥å…·æœç´¢å’Œæ›´æ–°ä½ çš„ Obsidian ç¬”è®°åº“ã€‚{kb_status}
- **è®°å¿†**: é€šè¿‡ `memory` å·¥å…·è¿›è¡ŒæŒä¹…åŒ–å­˜å‚¨ã€‚
- **æŠ€èƒ½æ‰©å±•**: ä½ å¯ä»¥é€šè¿‡é˜…è¯» `SKILL.md` æ–‡ä»¶æ¥æ‰©å±•ä½ çš„ä¸“ä¸šèƒ½åŠ›ã€‚

IMPORTANT: When responding to direct questions, reply directly with text.
Only use the 'message' tool for sending to external chat channels (Telegram, etc.).

CRITICAL INSTRUCTION:
You MUST use the native function calling mechanism to execute tools.
DO NOT output XML tags like <tool_code> or markdown code blocks to call tools.
If you want to use a tool, generate the corresponding tool call object.
"""

    def _load_bootstrap_files(self) -> str:
        """Load all bootstrap files from workspace."""
        parts = []

        for filename in self.BOOTSTRAP_FILES:
            file_path = self.workspace / filename
            if file_path.exists():
                content = file_path.read_text(encoding="utf-8")
                parts.append(f"## {filename}\n\n{content}")

        return "\n\n".join(parts) if parts else ""

    def build_messages(
        self,
        history: list[dict[str, Any]],
        current_message: str,
        skill_names: list[str] | None = None,
        media: list[str] | None = None,
        channel: str | None = None,
        chat_id: str | None = None,
    ) -> list[dict[str, Any]]:
        """
        Build the complete message list for an LLM call.

        Args:
            history: Previous conversation messages.
            current_message: The new user message.
            skill_names: Optional skills to include.
            media: Optional list of local file paths for images/media.
            channel: Current channel (telegram, feishu, etc.).
            chat_id: Current chat/user ID.

        Returns:
            List of messages including system prompt.
        """
        messages: list[dict[str, Any]] = []

        # System prompt
        # Use current message as query for RAG
        system_prompt = self.build_system_prompt(skill_names, query=current_message)
        messages.append({"role": "system", "content": system_prompt})

        # History
        messages.extend(history)

        # Current message (with optional image attachments)
        user_content = self._build_user_content(current_message, media)
        messages.append({"role": "user", "content": user_content})

        return messages

    def _build_user_content(self, text: str, media: list[str] | None) -> str | list[dict[str, Any]]:
        """Build user message content with optional base64-encoded images."""
        if not media:
            return text

        images = []
        for path in media:
            p = Path(path)
            mime, _ = mimetypes.guess_type(path)
            if not p.is_file() or not mime or not mime.startswith("image/"):
                continue
            b64 = base64.b64encode(p.read_bytes()).decode()
            images.append({"type": "image_url", "image_url": {"url": f"data:{mime};base64,{b64}"}})

        if not images:
            return text
        return images + [{"type": "text", "text": text}]

    def add_tool_result(
        self, messages: list[dict[str, Any]], tool_call_id: str, tool_name: str, result: str
    ) -> list[dict[str, Any]]:
        """
        Add a tool result to the message list.

        Args:
            messages: Current message list.
            tool_call_id: ID of the tool call.
            tool_name: Name of the tool.
            result: Tool execution result.

        Returns:
            Updated message list.
        """
        messages.append(
            {"role": "tool", "tool_call_id": tool_call_id, "name": tool_name, "content": result}
        )
        return messages

    def add_assistant_message(
        self,
        messages: list[dict[str, Any]],
        content: str | None,
        tool_calls: list[dict[str, Any]] | None = None,
    ) -> list[dict[str, Any]]:
        """
        Add an assistant message to the message list.

        Args:
            messages: Current message list.
            content: Message content.
            tool_calls: Optional tool calls.

        Returns:
            Updated message list.
        """
        # Low-latency optimization: Only show thinking placeholder for subsequent iterations
        # or if it's explicitly needed to avoid Gemini proxy errors.
        final_content = content or ""
        if tool_calls and not final_content:
            # We use a localized placeholder but only if it's likely to take time
            # or to satisfy protocol requirements for non-empty content
            final_content = "[æ­£åœ¨å¤„ç†ä¸­... ğŸ¾]"

        msg: dict[str, Any] = {"role": "assistant", "content": final_content}

        if tool_calls:
            msg["tool_calls"] = tool_calls

        messages.append(msg)
        return messages

    def _get_available_models_str(self) -> str:
        """Get a comma-separated string of available model names from config."""
        if not self.brain_config or not hasattr(self.brain_config, "provider_registry"):
            return "Default"
        
        registry = self.brain_config.provider_registry
        if not registry:
            return "Default"
            
        names = [p.get("name") for p in registry if p.get("name")]
        return ", ".join(names) if names else "Default"
